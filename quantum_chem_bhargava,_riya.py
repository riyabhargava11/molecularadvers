# -*- coding: utf-8 -*-
"""quantum chem - bhargava, riya

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1InZO00RJ_gf3IvpMTbZd2kzaM6sMy17p
"""

import random, numpy as np
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda)

SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if device == "cuda":
    torch.cuda.manual_seed_all(SEED)

import sys, subprocess

def pip_install(args):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + args)

pip_install(["--upgrade", "pip"])
pip_install([
    "--no-cache-dir", "--force-reinstall",
    "torch==2.4.0",
    "torchvision==0.19.0",
    "torchaudio==2.4.0",
    "--index-url", "https://download.pytorch.org/whl/cu121"
])

import torch
print("Pinned torch:", torch.__version__, "CUDA:", torch.version.cuda, "GPU:", torch.cuda.is_available())

print("\nIMPORTANT: Now do Runtime → Restart runtime, then continue with Cell 3.")

import torch
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "GPU:", torch.cuda.is_available())

import sys, subprocess

def pip_install(args):
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + args)

wheel_url = "https://data.pyg.org/whl/torch-2.4.0+cu121.html"
print("Using wheel index:", wheel_url)

pip_install(["--no-cache-dir", "--force-reinstall", "torch-scatter", "-f", wheel_url])
pip_install(["--no-cache-dir", "--force-reinstall", "torch-sparse",  "-f", wheel_url])
pip_install(["--no-cache-dir", "--force-reinstall", "torch-cluster", "-f", wheel_url])
pip_install(["--no-cache-dir", "--force-reinstall", "torch-spline-conv", "-f", wheel_url])
pip_install(["--no-cache-dir", "--force-reinstall", "torch-geometric"])

print("PyG installed.")

# cell objective — Imports

import time, random
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt

import torch_geometric
from torch_geometric.datasets import QM9
from torch_geometric.loader import DataLoader

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)
print("torch:", torch.__version__, "CUDA:", torch.version.cuda)
print("torch_geometric:", torch_geometric.__version__)

# cell objective: — Loading QM9 + splits

dataset = QM9(root="qm9_pyg")
print("QM9 size:", len(dataset))
print("Example: z", dataset[0].z.shape, "| pos", dataset[0].pos.shape, "| y", dataset[0].y.shape)

N_TRAIN = 2000
N_TEST  = 500

train_set = dataset[:N_TRAIN]
test_set  = dataset[N_TRAIN:N_TRAIN + N_TEST]

train_loader = DataLoader(train_set, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_set,  batch_size=32, shuffle=False)

TARGET_COL = 0
def get_target(batch):
    return batch.y[:, TARGET_COL]

# cell objective — Model w/ differentiable w.r.t. positions

class PairEnergyNet(nn.Module):
    def __init__(self, num_atom_types=100, emb_dim=32, hidden=64):
        super().__init__()
        self.emb = nn.Embedding(num_atom_types, emb_dim)
        self.mlp = nn.Sequential(
            nn.Linear(2 * emb_dim + 1, hidden),
            nn.SiLU(),
            nn.Linear(hidden, hidden),
            nn.SiLU(),
            nn.Linear(hidden, 1),
        )

    def forward(self, z, pos, batch):
        num_mols = int(batch.max().item()) + 1
        E = torch.zeros(num_mols, device=pos.device)

        for mol in range(num_mols):
            idx = (batch == mol).nonzero(as_tuple=False).squeeze(-1)
            p = pos[idx]
            a = z[idx]
            n = p.shape[0]
            if n < 2:
                continue

            ii, jj = torch.triu_indices(n, n, offset=1, device=pos.device)
            dist = (p[ii] - p[jj]).norm(dim=1, keepdim=True)

            ei = self.emb(a[ii])
            ej = self.emb(a[jj])
            x = torch.cat([ei, ej, dist], dim=1)
            e_pair = self.mlp(x).squeeze(-1)
            E[mol] = e_pair.sum()

        return E

model = PairEnergyNet().to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
print("Model ready.")

# cell objective —  training

model.train()
t0 = time.time()

for epoch in range(40):
    losses = []
    for batch in train_loader:
        batch = batch.to(device)
        pred = model(batch.z, batch.pos, batch.batch)
        y = get_target(batch)

        loss = F.l1_loss(pred, y)
        opt.zero_grad()
        loss.backward()
        opt.step()

        losses.append(loss.detach().item())

    print(f"epoch {epoch} | mean L1 loss: {np.mean(losses):.4f}")

print("Train minutes:", (time.time() - t0) / 60)

# cell objective — EDA 1 -- molecule size distribution

num_atoms = [d.z.shape[0] for d in dataset[:5000]]

plt.figure()
plt.hist(num_atoms, bins=20)
plt.xlabel("Number of atoms per molecule")
plt.ylabel("Count")
plt.title("QM9 molecule size distribution (first 5000)")
plt.show()

# cell objective: — EDA 2 interatomic distance distribution

dists = []
for d in dataset[:300]:
    pos = d.pos
    D = torch.cdist(pos, pos)
    mask = ~torch.eye(len(pos), dtype=torch.bool)
    dists.extend(D[mask].cpu().numpy())

plt.figure()
plt.hist(dists, bins=100, range=(0.5, 5.0))
plt.xlabel("Interatomic distance (Å)")
plt.ylabel("Count")
plt.title("Interatomic distance distribution (300 molecules)")
plt.show()

# cell objective: — EDA 3: Clean prediction vs target

model.eval()
preds, targets = [], []

with torch.no_grad():
    for batch in test_loader:
        batch = batch.to(device)
        pred = model(batch.z, batch.pos, batch.batch)
        y = get_target(batch)
        preds.extend(pred.cpu().numpy())
        targets.extend(y.cpu().numpy())
        if len(preds) >= 600:
            break

plt.figure()
plt.scatter(targets, preds, s=10, alpha=0.5)
plt.xlabel("True target (QM9 y[:,TARGET_COL])")
plt.ylabel("Predicted energy")
plt.title("Clean predictions vs targets (subset)")
lo = min(min(targets), min(preds))
hi = max(max(targets), max(preds))
plt.plot([lo, hi], [lo, hi], "--")
plt.show()

# cell objective: — EDA 4 -- Gradient norm distribution
model.eval()
batch = next(iter(test_loader)).to(device)

pos = batch.pos.clone().detach().requires_grad_(True)
E = model(batch.z, pos, batch.batch).sum()
E.backward()

g = pos.grad.norm(dim=1).detach().cpu().numpy()

plt.figure()
plt.hist(g, bins=50)
plt.xlabel(r"$\|\partial E / \partial r_i\|$")
plt.ylabel("Count")
plt.title("Per-atom gradient norm distribution (one batch)")
plt.show()

# cell objective: — RS-FGSM + PGD

@torch.no_grad()
def energy_pred(batch, pos_override=None):
    if pos_override is None:
        return model(batch.z, batch.pos, batch.batch)
    return model(batch.z, pos_override, batch.batch)

def rs_fgsm_deviation(batch, eps):
    batch = batch.to(device)
    with torch.no_grad():
        E_clean = energy_pred(batch).detach()

    pos_clean = batch.pos.clone().detach()

    # usinf random start inside the L_inf epsilon ball
    pos_adv = pos_clean + (2 * torch.rand_like(pos_clean) - 1.0) * eps
    pos_adv = pos_adv.clone().detach().requires_grad_(True)

    E_adv = model(batch.z, pos_adv, batch.batch)
    loss = ((E_adv - E_clean) ** 2).sum()
    loss.backward()
    grad = pos_adv.grad

    # One FGSM step + projection
    pos_adv = pos_adv.detach() + eps * grad.sign()
    delta = torch.clamp(pos_adv - pos_clean, -eps, eps)
    pos_adv = (pos_clean + delta).detach()

    return pos_clean, pos_adv, E_clean

def pgd_deviation(batch, eps, alpha, steps, random_start=True):
    batch = batch.to(device)
    with torch.no_grad():
        E_clean = energy_pred(batch).detach()

    pos_clean = batch.pos.clone().detach()

    if random_start:
        pos_adv = pos_clean + (2 * torch.rand_like(pos_clean) - 1.0) * eps
    else:
        pos_adv = pos_clean.clone()

    delta = torch.clamp(pos_adv - pos_clean, -eps, eps)
    pos_adv = (pos_clean + delta).detach()

    for _ in range(steps):
        pos_adv = pos_adv.clone().detach().requires_grad_(True)
        E_adv = model(batch.z, pos_adv, batch.batch)
        loss = ((E_adv - E_clean) ** 2).sum()
        loss.backward()
        grad = pos_adv.grad

        pos_adv = pos_adv.detach() + alpha * grad.sign()
        delta = torch.clamp(pos_adv - pos_clean, -eps, eps)
        pos_adv = (pos_clean + delta).detach()

    return pos_clean, pos_adv, E_clean

# cell onjective:  run attacks + figures generation
#                     - add sensitivity vs molecule size plot

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

model.eval()

eps_list = [0.002, 0.005, 0.01, 0.02]
PGD_STEPS = 5
MAX_MOLS  = 250

rows = []
mols_seen = 0

for batch in test_loader:
    batch = batch.to(device)

    # Computinf atoms per molecule for this batch (vector length = num_graphs)
    # batch.batch maps each atom corresponds to each molecule id
    n_graphs = batch.num_graphs
    n_atoms = torch.bincount(batch.batch, minlength=n_graphs).detach().cpu().numpy()

    for eps in eps_list:

        # RS-FGSM

        _, pos_adv, E_clean = rs_fgsm_deviation(batch, eps)
        with torch.no_grad():
            E_adv = model(batch.z, pos_adv, batch.batch)

        dE = (E_adv - E_clean).abs().detach().cpu().numpy()
        dE_per_atom = dE / n_atoms

        for i in range(n_graphs):
            rows.append({
                "attack": "RS-FGSM",
                "eps": eps,
                "num_atoms": int(n_atoms[i]),
                "dE_abs": float(dE[i]),
                "dE_abs_per_atom": float(dE_per_atom[i]),
            })


        # PGD

        alpha = eps / PGD_STEPS
        _, pos_adv2, E_clean2 = pgd_deviation(batch, eps, alpha=alpha, steps=PGD_STEPS, random_start=True)
        with torch.no_grad():
            E_adv2 = model(batch.z, pos_adv2, batch.batch)

        dE2 = (E_adv2 - E_clean2).abs().detach().cpu().numpy()
        dE2_per_atom = dE2 / n_atoms

        for i in range(n_graphs):
            rows.append({
                "attack": f"PGD-{PGD_STEPS}",
                "eps": eps,
                "num_atoms": int(n_atoms[i]),
                "dE_abs": float(dE2[i]),
                "dE_abs_per_atom": float(dE2_per_atom[i]),
            })

    mols_seen += batch.num_graphs
    if mols_seen >= MAX_MOLS:
        break

df = pd.DataFrame(rows)
print(df.head())
print("Rows:", len(df), "Molecules evaluated (approx):", mols_seen)


# Figure A: mean |ΔE| vs epsilon (raw)

summary_raw = df.groupby(["attack", "eps"])["dE_abs"].mean().reset_index()

plt.figure()
for attack in summary_raw.attack.unique():
    sub = summary_raw[summary_raw.attack == attack].sort_values("eps")
    plt.plot(sub.eps, sub.dE_abs, marker="o", label=attack)
plt.xlabel("epsilon (Å)")
plt.ylabel("mean |ΔE| (raw)")
plt.title("Adversarial effect size vs epsilon (raw |ΔE|)")
plt.legend()
plt.show()


# Figure B: mean |ΔE| vs epsilon (per-atom)

summary_atom = df.groupby(["attack", "eps"])["dE_abs_per_atom"].mean().reset_index()

plt.figure()
for attack in summary_atom.attack.unique():
    sub = summary_atom[summary_atom.attack == attack].sort_values("eps")
    plt.plot(sub.eps, sub.dE_abs_per_atom, marker="o", label=attack)
plt.xlabel("epsilon (Å)")
plt.ylabel("mean |ΔE| per atom")
plt.title("Adversarial effect size vs epsilon (per-atom normalized)")
plt.legend()
plt.show()


# Figure C: distribution at fixed epsilon (per-atom)

eps_focus = 0.01
sub = df[df.eps == eps_focus]

plt.figure()
for attack in sub.attack.unique():
    vals = sub[sub.attack == attack]["dE_abs_per_atom"].values
    plt.hist(vals, bins=40, alpha=0.5, label=attack)
plt.xlabel("|ΔE| per atom")
plt.ylabel("Count")
plt.title(f"|ΔE| per atom distribution at ε={eps_focus}")
plt.legend()
plt.show()


# Figure D: sensitivity vs molecule size (per-atom)

plt.figure()
for attack in sub.attack.unique():
    ss = sub[sub.attack == attack]
    plt.scatter(ss["num_atoms"], ss["dE_abs_per_atom"], alpha=0.35, s=12, label=attack)
plt.xlabel("Number of atoms")
plt.ylabel("|ΔE| per atom")
plt.title(f"Adversarial sensitivity vs molecule size (ε={eps_focus})")
plt.legend()
plt.show()

df.to_csv("qm9_attacks_per_molecule.csv", index=False)
summary_raw.to_csv("qm9_attacks_summary_raw.csv", index=False)
summary_atom.to_csv("qm9_attacks_summary_per_atom.csv", index=False)
print("Saved CSVs:")
print(" - qm9_attacks_per_molecule.csv")
print(" - qm9_attacks_summary_raw.csv")
print(" - qm9_attacks_summary_per_atom.csv")